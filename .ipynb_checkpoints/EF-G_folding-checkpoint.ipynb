{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943184eb-b14a-4000-a256-df92cb963e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used *Evolutionary Insights into Elongation Factor G Using AlphaFold and Ancestral Analysis*.  \n",
    "**[Computers in Biology and Medicine]**, Accepted.\n",
    "# most the cells can be used independently with custom input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99a00f-e1c0-4b9b-8472-f6823cbee7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations\n",
    "# run with colabfold 1.5.5 from : https://github.com/sokrypton/ColabFold (Mirdita, M., Schutze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S., and Steinegger, M. (2022). ColabFold: making protein folding accessible to all. Nat Methods 19, 679-682. 10.1038/s41592-022-01488-1.)\n",
    "# followed clustering algorithm from : https://github.com/HWaymentSteele/AF_Cluster (Wayment-Steele, H.K., Ojoawo, A., Otten, R., Apitz, J.M., Pitsawong, W., Homberger, M., Ovchinnikov, S., Colwell, L., and Kern, D. (2024). Predicting multiple conformations via sequence clustering and AlphaFold2. Nature 625, 832-839. 10.1038/s41586-023-06832-9.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46badc-b92e-40df-9f7a-939d3af45976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 set consistent directions.\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "default_path = \"/home/yuhong/G_Fold\"\n",
    "##########################################################################################2\n",
    "COLAB = \"/home/yuhong/colabfold/localcolabfold/colabfold-conda/bin/colabfold_batch\"\n",
    "SEQ = os.path.join(default_path,\"SEQ\")\n",
    "os.makedirs(SEQ, exist_ok=True)\n",
    "###########################################################################################\n",
    "##########################################################################################3\n",
    "# set the right a3m to cluster. should have just one in the folder\n",
    "a3m_files = glob.glob(os.path.join(SEQ, \"*.a3m\"))\n",
    "input_path = os.path.join(default_path,\"Cluster\")\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "###########################################################################################\n",
    "##########################################################################################4\n",
    "# define input and output file path \n",
    "a3m_path = os.path.join(input_path,\"folding\")\n",
    "os.makedirs(a3m_path, exist_ok=True)\n",
    "# Set the size threshold (4 KB)\n",
    "size_threshold = 4 * 1024  # 4 KB for automatic operation. Choose based on sequence length. \n",
    "###########################################################################################\n",
    "##########################################################################################5\n",
    "# Define the log file path, CSV path, PDB directories, and threshold\n",
    "log_file_path = os.path.join (a3m_path,\"log.txt\")  # Replace with the actual path\n",
    "csv_output_path = os.path.join(a3m_path,\"pLDDT.csv\")  # Path for the output CSV file\n",
    "pLDDT_threshold = 60  # Set your threshold for pLDDT\n",
    "pdb_source_dir = a3m_path  # Directory containing the PDB files\n",
    "pdb_target_dir = os.path.join(a3m_path,\"sel_pdb\")  # Directory to move selected PDB files\n",
    "os.makedirs(pdb_target_dir, exist_ok=True)\n",
    "###########################################################################################\n",
    "##########################################################################################6 \n",
    "residue_range1 = \"1-286\" # make sure only set value onsite\n",
    "first_PSE_folder = os.path.join(pdb_target_dir,f\"1st_{residue_range1}_RMSD\")\n",
    "os.makedirs(first_PSE_folder, exist_ok=True)                                \n",
    "first_PSE = os.path.join(first_PSE_folder,f\"1st_{residue_range1}_RMSD.pse\")                         \n",
    "###########################################################################################\n",
    "##########################################################################################7\n",
    "session_path = first_PSE  # direct run\n",
    "rmsd_references = [\"EX_175\", \"EX_276\"]  # List of reference PDBs (without .pdb extension)\n",
    "###########################################################################################\n",
    "###########################################################################################8\n",
    "plddt_file = os.path.join(a3m_path,\"pLDDT.csv\")\n",
    "selected_headers = ['EX_175', 'EX_276']\n",
    "############################################################################################\n",
    "###########################################################################################9\n",
    "a3m_directory = input_path\n",
    "list1_path = os.path.join(os.path.dirname(session_path),\"WT_list1.txt\")\n",
    "list2_path = list2_path = list1_path.replace(\"WT_list1.txt\", \"WT_list2.txt\")\n",
    "prefix = os.path.basename(list1_path).split('_')[0]\n",
    "final_combined_a3m = os.path.join(os.path.dirname(list1_path), f\"{prefix}_ForTree.fasta\")\n",
    "###########################################################################################\n",
    "##########################################################################################10\n",
    "# fetch the protein ID with the previous output.\n",
    "TAX_FA = final_combined_a3m.replace(\".fasta\",\"_TAX.fa\")\n",
    "###########################################################################################\n",
    "#########################################################################################11\n",
    "#TAX_FA = final_combined_a3m.replace(\".fasta\",\"_TAX.fa\")\n",
    "###########################################################################################\n",
    "##########################################################################################12\n",
    "# same input TAX_FA\n",
    "###########################################################################################\n",
    "##########################################################################################13\n",
    "#define tree methods.\n",
    "###########################################################################################\n",
    "###########################################################################################14\n",
    "TAX_Shortened_FAS = TAX_FA.replace(\".fa\",\"_shortened.fas\")  \n",
    "\n",
    "###########################################################################################\n",
    "#########################################################################################14b\n",
    "directory_for_tree =os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "##########################################################################################\n",
    "##########################################################################################15\n",
    "# Path to the directory containing .treefile files\n",
    "directory_path = os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "###########################################################################################\n",
    "##########################################################################################16\n",
    "# Path to the directory containing .treefile files\n",
    "directory_path = os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "###########################################################################################\n",
    "#########################################################################################17\n",
    "#it generates 1-unique, 2-unique, 1/2-common; then continue with 3 to generate 3x3 more txt, and so on.\n",
    "# For files with 0Kb, or other threshold, it will not get into the next round of clustering. \n",
    "# refer to Figures S4 and S10 in  \n",
    "##########################################################################################\n",
    "########################################################################################18\n",
    "# input folder is first_PSE\n",
    "node_branch_csv = os.path.join(first_PSE, \"internal_nodes_distances.csv\")\n",
    "##########################################################################################\n",
    "########################################################################################19        \n",
    "disfile = node_branch_csv\n",
    "node_thr = 3.4\n",
    "matrix_dir = os.path.join(first_PSE, \"matrix\")\n",
    "##########################################################################################\n",
    "########################################################################################20\n",
    "output_jsd_folder = os.path.join(matrix_dir, \"JSD\") # matrix_dir  \n",
    "final_output_filename = os.path.join(output_jsd_folder, \"node_JSD.csv\")\n",
    "##########################################################################################\n",
    "########################################################################################21\n",
    "# Define the path to the saved JSD matrix\n",
    "node_jsd_path = os.path.join(output_jsd_folder, \"node_JSD.csv\")\n",
    "\n",
    "# Load the saved JSD matrix. this is more reliable to read the .csv, which can be slow for large node list. \n",
    "if os.path.exists(node_jsd_path):\n",
    "    node_jsd = pd.read_csv(node_jsd_path, index_col=0)  # Load as DataFrame\n",
    "    print(\"✅ Successfully loaded node_JSD matrix without recomputing!\")\n",
    "\n",
    "    # Fix NaN values in the diagonal by replacing them with 1\n",
    "    np.fill_diagonal(node_jsd.values, 1)\n",
    "\n",
    "    # Overwrite the original file with the fixed matrix\n",
    "    node_jsd.to_csv(node_jsd_path)\n",
    "    print(f\"✅ Fixed and saved node_JSD matrix to: {node_jsd_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️ File not found: {node_jsd_path}\")\n",
    "\n",
    "##########################################################################################\n",
    "########################################################################################22\n",
    "#clustered_csv = hierarchical_clustering(node_jsd_path)[0] from 21\n",
    "##########################################################################################\n",
    "########################################################################################23\n",
    "logo_dir = \"/home/yuhong/LOGO\"  # Path containing aligned sequences\n",
    "valid_extension = \".fasta\"  # Specify the desired extension to process\n",
    "start_pos = 1  # Start position (1-based index)\n",
    "end_pos = -1   # End position (inclusive, set to -1 for end of sequence)\n",
    "chunk_size = 40  # Size of each fragment\n",
    "width = 0.5  # Width of each letter; less than 1.0 increases spacing between letters\n",
    "font_weight = 'light'  # Font weight for the letters ('light', 'normal', 'bold', etc.)\n",
    "##########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa08a32-25d2-4b3d-b0c4-65d8a2762f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 move query sequence into SEQ then run colabfold\n",
    "# path of colabfold_batch is: /home/yuhong/colabfold/localcolabfold/colabfold-conda/bin/colabfold_batch\n",
    "#set short reference to colabfold_batch\n",
    "import os\n",
    "###########################################################################################\n",
    "COLAB = \"/home/yuhong/colabfold/localcolabfold/colabfold-conda/bin/colabfold_batch\"\n",
    "SEQ = os.path.join(default_path,\"SEQ\")\n",
    "os.makedirs(SEQ, exist_ok=True)\n",
    "###########################################################################################\n",
    "!$COLAB --num-recycle 3 --num-models 3 --max-seq 1024 --max-extra-seq 10 {SEQ} {SEQ}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45fa3e-1834-4450-a531-d09f87b383d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 cluster .a3m in SEQ/. Then move them into Cluster\n",
    "# need ClusterMSA_min3.py and utils.py ; both in default_path\n",
    "import os\n",
    "import glob\n",
    "##########################################################################################3\n",
    "# set the right a3m to cluster. should have just one in the folder\n",
    "a3m_files = glob.glob(os.path.join(SEQ, \"*.a3m\"))\n",
    "if a3m_files:\n",
    "    a3m_for_cluster = a3m_files[0]  # Select the first file\n",
    "    print(f\"Selected A3M file for clustering: {a3m_for_cluster}\")\n",
    "else:\n",
    "    print(\"No .a3m files found in SEQ/\")\n",
    "# The clustered a3m moved to {default_path}/Cluster, which will be input for the folding and pdb sorting. \n",
    "input_path = os.path.join(default_path,\"Cluster\")\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "#############################################################################################\n",
    "!python $default_path/ClusterMSA_min3.py EX -i {a3m_for_cluster} -o {input_path}  --log_dir {input_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333e3a1-1aa5-495e-80ac-980ab95b9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. source input_path = os.path.join(default_path,\"Cluster\")\n",
    "# new path in this cell: a3m_path = os.path.join(input_path,\"folding\")\n",
    "#move random chosen half to fold. This is not necessary when commputing with GPU later on. \n",
    "\n",
    "import os\n",
    "import shutil\n",
    "##########################################################################4\n",
    "# define input and output file path \n",
    "a3m_path = os.path.join(input_path,\"folding\")\n",
    "os.makedirs(a3m_path, exist_ok=True)\n",
    "# Set the size threshold (4 KB)\n",
    "size_threshold = 4 * 1024  # 4 KB for automatic operation. Choose based on sequence length. \n",
    "#########################################################################\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(input_path):\n",
    "    # Check if the file is a .a3m file\n",
    "    if filename.endswith(\".a3m\"):\n",
    "        file_path = os.path.join(input_path, filename)\n",
    "        # Get the size of the file in bytes\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        # If the file size is less than the threshold, remove the file\n",
    "        if file_size >= size_threshold:\n",
    "            print(f\"moving {filename} (Size: {file_size} bytes)\")\n",
    "            shutil.move(file_path, os.path.join(a3m_path, filename))\n",
    "\n",
    "print(\"move complete!\")\n",
    "\n",
    "# folding all these a3m files. \n",
    "!$COLAB {a3m_path} {a3m_path} --num-models 1 --max-seq 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5bf16-c396-4fc9-b650-0792262cdf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. make selected pdb with pLDDT > threshold. If pLDDT already exist, skip the extracting step. \n",
    "# source: a3m_path\n",
    "# outputpdb_target_dir = os.path.join(a3m_path,\"sel_pdb\")\n",
    "#csv_output_path = os.path.jpoin(a3m_path,\"pLDDT.csv\")\n",
    "# searching for best pLDDT > threshold, then move those into sel_pdb folder. \n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "##########################################################################################################5\n",
    "# Define the log file path, CSV path, PDB directories, and threshold\n",
    "log_file_path = os.path.join (a3m_path,\"log.txt\")  # Replace with the actual path\n",
    "csv_output_path = os.path.join(a3m_path,\"pLDDT.csv\")  # Path for the output CSV file\n",
    "pLDDT_threshold = 58.4  # Set your threshold for pLDDT\n",
    "pdb_source_dir = a3m_path  # Directory containing the PDB files\n",
    "pdb_target_dir = os.path.join(a3m_path,\"sel_pdb\")  # Directory to move selected PDB files\n",
    "os.makedirs(pdb_target_dir, exist_ok=True)\n",
    "#############################################################################################################\n",
    "\n",
    "def process_and_move_pdb_files(source_dir, target_dir, threshold_val, log_path, csv_path):\n",
    "    # If CSV already exists, skip log parsing\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"pLDDT.csv not found. Extracting from log.txt...\")\n",
    "\n",
    "        with open(log_path, 'r') as log_file:\n",
    "            lines = log_file.readlines()\n",
    "\n",
    "        extracted_data = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"rank_001\" in line:\n",
    "                pLDDT_match = re.search(r'pLDDT=([\\d.]+)', line)\n",
    "                if pLDDT_match:\n",
    "                    pLDDT_value = float(pLDDT_match.group(1))\n",
    "                    for j in range(i, -1, -1):\n",
    "                        if \"Query\" in lines[j]:\n",
    "                            ex_match = re.search(r'(EX_\\w{3})', lines[j])\n",
    "                            if ex_match:\n",
    "                                ex_code = ex_match.group(0)\n",
    "                                extracted_data.append([ex_code, pLDDT_value])\n",
    "                                break\n",
    "\n",
    "        with open(csv_path, 'w', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(['EX_Code', 'pLDDT_Value'])\n",
    "            writer.writerows(extracted_data)\n",
    "\n",
    "        print(f\"Extracted data saved to {csv_path}\")\n",
    "    else:\n",
    "        print(\"pLDDT.csv already exists. Skipping log.txt processing.\")\n",
    "\n",
    "    # Load and process the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sorted_df = df.sort_values(by='pLDDT_Value', ascending=False)\n",
    "    truncated_df = sorted_df[sorted_df['pLDDT_Value'] >= threshold_val]\n",
    "\n",
    "    truncated_csv_path = os.path.join(target_dir, \"pLDDT_truncated.csv\")\n",
    "    truncated_df.to_csv(truncated_csv_path, index=False)\n",
    "    print(f\"Filtered pLDDT values saved to: {truncated_csv_path}\")\n",
    "\n",
    "    # Move and rename corresponding PDBs\n",
    "    for _, row in truncated_df.iterrows():\n",
    "        ex_code = row['EX_Code']\n",
    "        matched = False\n",
    "        for file_name in os.listdir(source_dir):\n",
    "            if file_name.startswith(f\"{ex_code}_unrelaxed_rank_001\"): \n",
    "                source_path = os.path.join(source_dir, file_name)\n",
    "                target_path = os.path.join(target_dir, f\"{ex_code}.pdb\")\n",
    "                shutil.move(source_path, target_path)\n",
    "                print(f\"Moved and renamed: {file_name} -> {ex_code}.pdb\")\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            print(f\"File not found for EX code: {ex_code}\")\n",
    "\n",
    "    print(\"PDB file processing complete.\")\n",
    "\n",
    "# Run it\n",
    "process_and_move_pdb_files(pdb_source_dir, pdb_target_dir, pLDDT_threshold, log_file_path, csv_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f71c2b-03a3-43ab-9625-5a7519478264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 load all pdbs in the default_path (or in subfolders if contain pdbs) to pymol2, align with ref in a range.This is more efficient because \n",
    "# almost prefectly aligned. so the RMSD reflects more on the true divergences in other ranges. \n",
    "# save the new pdbs coordinates in the\n",
    "#pse. and in a new subfolder: /aligned. then output the RMSD.\n",
    "# need to install pymol. \n",
    "\n",
    "import os\n",
    "import pymol2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "############################################################################### 6 \n",
    "residue_range1 = \"1-286\" # change range if need\n",
    "NameOfFolder = f\"1st_{residue_range1}_RMSD\"# make a subfolder and pse and RMSD all named the same in the folder.\n",
    "# pdb_target_dir = os.path.join(a3m_path,\"sel_pdb\")  # Directory to move selected PDB files\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def find_reference_pdb(parent_folder):\n",
    "    \"\"\"Locate the reference PDB file by searching in the parent folder and its subfolders.\"\"\"\n",
    "    \n",
    "    PDB_list_path = os.path.join(parent_folder, \"pLDDT_truncated.csv\")\n",
    "    try:\n",
    "        REF_pdb_base = subprocess.run(\n",
    "            f\"tail -n +2 {PDB_list_path} | head -n 1 | cut -d, -f1\",\n",
    "            shell=True, text=True, capture_output=True, check=True\n",
    "        ).stdout.strip()\n",
    "\n",
    "        if not REF_pdb_base:\n",
    "            print(\"Error: No valid reference PDB found in pLDDT_truncated.csv.\")\n",
    "            return None\n",
    "\n",
    "        REF_pdb_name = REF_pdb_base + \".pdb\"\n",
    "        REF_pdb_path = os.path.join(parent_folder, REF_pdb_name)\n",
    "\n",
    "        if os.path.exists(REF_pdb_path):\n",
    "            return REF_pdb_path\n",
    "\n",
    "        pdb_prefix = REF_pdb_base.split(\"_\")[0]\n",
    "        for root, _, files in os.walk(parent_folder):\n",
    "            if os.path.basename(root).startswith(pdb_prefix):\n",
    "                for file in files:\n",
    "                    if file == REF_pdb_name:\n",
    "                        return os.path.join(root, file)\n",
    "\n",
    "        print(f\"Error: Reference PDB {REF_pdb_name} not found in parent folder or subfolders.\")\n",
    "        return None\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Error: Could not extract reference PDB from pLDDT_truncated.csv.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_align_pdbs(parent_folder, resi_range):\n",
    "    \"\"\"\n",
    "    Load all PDB files from the parent folder (or subfolders), align them to a reference PDB,\n",
    "    save the new coordinates in a subfolder /aligned, and calculate the RMSD matrix only for PDBs in the /aligned folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    REF_pdb = find_reference_pdb(parent_folder)\n",
    "    if REF_pdb is None:\n",
    "        return None, None\n",
    "\n",
    "    REF_pdb_base = os.path.basename(REF_pdb).replace(\".pdb\", \"\")\n",
    "\n",
    "    with pymol2.PyMOL() as pymol:\n",
    "        pymol.cmd.reinitialize()\n",
    "\n",
    "        # Load reference PDB\n",
    "        print(f\"Loading reference PDB: {REF_pdb}\")\n",
    "        pymol.cmd.load(REF_pdb, \"REF\")\n",
    "        pymol.cmd.select(\"REF_align\", f\"REF and resi {resi_range}\")\n",
    "\n",
    "        aligned_folder = os.path.join(default_path,f\"1st_{residue_range1}_aligned_pdb\")\n",
    "        os.makedirs(aligned_folder, exist_ok=True)\n",
    "\n",
    "        # Load all PDB files from parent folder and subfolders\n",
    "        print(\"Searching for PDB files...\")\n",
    "        pdb_files = []\n",
    "        for root, _, files in os.walk(parent_folder):\n",
    "            for file_name in files:\n",
    "                if file_name.endswith(\".pdb\"):\n",
    "                    pdb_path = os.path.join(root, file_name)\n",
    "                    obj_name = os.path.splitext(file_name)[0]\n",
    "                    pymol.cmd.load(pdb_path, obj_name)\n",
    "                    pdb_files.append(obj_name)\n",
    "                    print(f\"Loaded: {obj_name}\")\n",
    "\n",
    "        if not pdb_files:\n",
    "            print(\"No PDB files found. Exiting...\")\n",
    "            return None, None\n",
    "\n",
    "        # Align all PDBs to the reference\n",
    "        print(\"Aligning PDB files...\")\n",
    "        for idx, obj_name in enumerate(pdb_files, start=1):\n",
    "            pymol.cmd.cealign(\"REF_align\",f\"{obj_name} and resi {resi_range}\")\n",
    "            output_pdb_path = os.path.join(aligned_folder, f\"{obj_name}.pdb\")\n",
    "            pymol.cmd.save(output_pdb_path, obj_name)\n",
    "            print(f\"Aligned {idx}/{len(pdb_files)}: {obj_name}, saved to {output_pdb_path}\")\n",
    "\n",
    "       # Remove reference from session\n",
    "        pymol.cmd.delete(\"ref\")\n",
    "        pymol.cmd.delete(\"ref_align\")\n",
    "\n",
    "\n",
    "        # Save PyMOL session in a separate folder parallel to pdb path,same as the RMSD matrix.\n",
    "        pse_folder = os.path.join(parent_folder,NameOfFolder)\n",
    "        os.makedirs(pse_folder,exist_ok=True)\n",
    "        pse_path = os.path.join(pse_folder, f\"{NameOfFolder}.pse\")\n",
    "        pymol.cmd.save(pse_path)\n",
    "        print(f\"PyMOL session saved as: {pse_path}\")\n",
    "\n",
    "        return pse_path\n",
    "\n",
    "# Run alignment\n",
    "first_PSE = load_and_align_pdbs(pdb_target_dir, residue_range1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca9d39-cc08-4ea3-b114-5aa84923e8b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#7. calculate RMSD toward EX_276 and EX_175, the highest of the two major folding. \n",
    "\n",
    "import os\n",
    "import pymol2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##################################################################################7\n",
    "try:\n",
    "    session_path = first_PSE\n",
    "except NameError:\n",
    "    session_path = \"/home/yuhong/G_Fold/Cluster/folding/sel_pdb/1st_1-286_RMSD/1st_1-286_RMSD.pse\"  # direct run\n",
    "rmsd_references = [\"EX_175\", \"EX_276\"]  # List of reference PDBs (without .pdb extension)\n",
    "##################################################################################\n",
    "\n",
    "def calculate_rmsd_from_pse(session_path, rmsd_references=None):\n",
    "    \"\"\"\n",
    "    Loads a PyMOL session (.pse), extracts all PDB structures, and calculates RMSD \n",
    "    against a list of reference PDBs.\n",
    "\n",
    "    Args:\n",
    "        session_path (str): Full path to the PyMOL session (.pse) file.\n",
    "        rmsd_references (list of str): List of reference object names (no path or extension).\n",
    "    \n",
    "    Output:\n",
    "        Saves an RMSD matrix as a CSV file in the same directory as the .pse file.\n",
    "    \"\"\"\n",
    "\n",
    "    session_path = os.path.abspath(session_path)\n",
    "    session_dir = os.path.dirname(session_path)\n",
    "    rmsd_output_path = os.path.join(session_dir, f\"{os.path.basename(session_path).replace('.pse', '.csv')}\")\n",
    "\n",
    "    with pymol2.PyMOL() as pymol:\n",
    "        pymol.cmd.reinitialize()\n",
    "        print(f\"Loading PyMOL session from: {session_path}\")\n",
    "        pymol.cmd.load(session_path)\n",
    "\n",
    "        pdb_objects = pymol.cmd.get_object_list()\n",
    "        print(f\"Total objects in session: {len(pdb_objects)}\")\n",
    "\n",
    "        if not rmsd_references:\n",
    "            rmsd_references = pdb_objects\n",
    "\n",
    "        n = len(pdb_objects)\n",
    "        m = len(rmsd_references)\n",
    "        rmsd_matrix = np.zeros((n, m))\n",
    "\n",
    "        print(\"Calculating RMSDs...\")\n",
    "        for i, obj1 in enumerate(pdb_objects):\n",
    "            for j, obj2 in enumerate(rmsd_references):\n",
    "                rmsd = pymol.cmd.rms_cur(obj1, obj2)\n",
    "                rmsd_matrix[i, j] = rmsd\n",
    "                print(f\"RMSD {obj1} vs {obj2} = {rmsd:.3f}\")\n",
    "\n",
    "        # Use only names (no path or extension)\n",
    "        row_labels = [os.path.basename(obj) for obj in pdb_objects]\n",
    "        col_labels = [os.path.basename(ref) for ref in rmsd_references]\n",
    "\n",
    "        rmsd_df = pd.DataFrame(rmsd_matrix, index=row_labels, columns=col_labels)\n",
    "        rmsd_df.to_csv(rmsd_output_path)\n",
    "        print(f\"Saved RMSD CSV to: {rmsd_output_path}\")\n",
    "        return rmsd_output_path  # Return the CSV path\n",
    "\n",
    "rmsd_matrix_file = calculate_rmsd_from_pse(session_path, rmsd_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4843ace-d1cd-4759-9ef4-89a1c23bf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. plot RMSD and color with pLDDT values. \n",
    "###########################################################\n",
    "plddt_file = os.path.join(a3m_path,\"pLDDT.csv\")\n",
    "selected_headers = ['EX_175', 'EX_276']\n",
    "###########################################################\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rmsd_vs_plddt(rmsd_matrix_file, plddt_file, selected_headers, x_range=None, y_range=None):\n",
    "    \"\"\"\n",
    "    Plots RMSD values from an RMSD matrix file against two selected references, \n",
    "    with points colored based on pLDDT values. Matches IDs with or without '.pdb'.\n",
    "\n",
    "    Args:\n",
    "        rmsd_matrix_file (str): Path to the RMSD matrix CSV file.\n",
    "        plddt_file (str): Path to the pLDDT CSV file.\n",
    "        selected_headers (list of str): Two column names to use as x and y axes.\n",
    "        x_range (tuple, optional): (min, max) values for the x-axis.\n",
    "        y_range (tuple, optional): (min, max) values for the y-axis.\n",
    "    \n",
    "    Output:\n",
    "        - Saves a merged CSV with pLDDT values.\n",
    "        - Displays an RMSD scatter plot colored by pLDDT values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV files\n",
    "    plddt_df = pd.read_csv(plddt_file)\n",
    "    rmsd_matrix_df = pd.read_csv(rmsd_matrix_file)\n",
    "\n",
    "    # Ensure the selected headers exist\n",
    "    if not all(header in rmsd_matrix_df.columns for header in selected_headers):\n",
    "        raise ValueError(f\"Selected headers {selected_headers} not found in RMSD matrix file.\")\n",
    "\n",
    "    # Select required columns from RMSD matrix\n",
    "    columns_to_select = [rmsd_matrix_df.columns[0]] + selected_headers  # Always include the first column\n",
    "    selected_rmsd_df = rmsd_matrix_df[columns_to_select]\n",
    "\n",
    "    # Function to strip '.pdb' if present\n",
    "    def strip_pdb(filename):\n",
    "        return filename.replace('.pdb', '') if isinstance(filename, str) else filename\n",
    "\n",
    "    # Apply stripping function to the first column of both files\n",
    "    selected_rmsd_df.iloc[:, 0] = selected_rmsd_df.iloc[:, 0].apply(strip_pdb)\n",
    "    plddt_df.iloc[:, 0] = plddt_df.iloc[:, 0].apply(strip_pdb)\n",
    "\n",
    "    # Map pLDDT values to RMSD rows based on matching stripped names\n",
    "    plddt_mapping = plddt_df.set_index(plddt_df.columns[0])  # Create lookup table\n",
    "    selected_rmsd_df['pLDDT'] = selected_rmsd_df.iloc[:, 0].map(plddt_mapping.iloc[:, 0])\n",
    "\n",
    "    # Save the new CSV\n",
    "    output_file = os.path.join(os.path.dirname(rmsd_matrix_file), 'RMSD_REF.csv')\n",
    "    selected_rmsd_df.to_csv(output_file, index=False)\n",
    "    print(f'New CSV file saved as {output_file}')\n",
    "\n",
    "    # Define RGB colors based on pLDDT value ranges\n",
    "    def assign_color(value):\n",
    "        if value >= 69.5:\n",
    "            return 'darkblue'\n",
    "        elif 59.5 <= value < 69.5:\n",
    "            return 'red'\n",
    "        elif 50 <= value < 60:\n",
    "            return 'white'\n",
    "        elif 40 <= value < 50:\n",
    "            return 'white'\n",
    "        else:\n",
    "            return 'white'  # Default for missing or out-of-range values\n",
    "\n",
    "    # Apply color assignment\n",
    "    selected_rmsd_df['color'] = selected_rmsd_df['pLDDT'].apply(assign_color)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        selected_rmsd_df[selected_headers[0]],  # X-axis\n",
    "        selected_rmsd_df[selected_headers[1]],  # Y-axis\n",
    "        c=selected_rmsd_df['color'],            # Colors based on pLDDT\n",
    "        alpha=0.8, edgecolors='k'\n",
    "    )\n",
    "\n",
    "    # Set axis limits if provided\n",
    "    if x_range:\n",
    "        plt.xlim(x_range)\n",
    "    if y_range:\n",
    "        plt.ylim(y_range)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(selected_headers[0], fontsize=12)\n",
    "    plt.ylabel(selected_headers[1], fontsize=12)\n",
    "    plt.title('RMSD vs Reference Colored by pLDDT Value', fontsize=14)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_rmsd_vs_plddt(rmsd_matrix_file, plddt_file, selected_headers,x_range=(0, 40), y_range=(0, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3eae7-1ba7-4493-8314-1c2e84b83d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. pool list 1 and list 2 after RMSD clustering. \n",
    "# set the WT_list1 and WT_list2 via supervision, saving in the same folder as the RMSD. \n",
    "# save those into the sel_pdb folder. \n",
    "# combine .a3m in (/home/yuhong/demo/SEQ/Cluster): input_path \n",
    "# and save the combinded taged .a3m in the same folder as the lists.\n",
    "\n",
    "import os\n",
    "###########################################################################################9\n",
    "a3m_directory = input_path\n",
    "list1_path = os.path.join(os.path.dirname(session_path),\"WT_list1.txt\")\n",
    "list2_path = list2_path = list1_path.replace(\"WT_list1.txt\", \"WT_list2.txt\")\n",
    "###########################################################################################\n",
    "\n",
    "def combine_a3m_files(list_file_path, a3m_directory):\n",
    "    \"\"\"\n",
    "    Combines sequences from .a3m files listed in a .txt file (which contains .pdb names).\n",
    "    Saves combined output as a .a3m file in the same folder as the .txt file.\n",
    "    \"\"\"\n",
    "    base_name = os.path.splitext(os.path.basename(list_file_path))[0]\n",
    "\n",
    "    # Read .pdb names and convert to .a3m\n",
    "    with open(list_file_path, 'r') as f:\n",
    "        a3m_filenames = [line.strip().replace('.pdb', '.a3m') for line in f if line.strip()]\n",
    "\n",
    "    combined = []\n",
    "    for idx, fname in enumerate(a3m_filenames):\n",
    "        fpath = os.path.join(a3m_directory, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            print(f\"Warning: {fpath} not found. Skipping.\")\n",
    "            continue\n",
    "        with open(fpath, 'r') as a3m_file:\n",
    "            lines = a3m_file.readlines()\n",
    "            combined.extend(lines if idx == 0 else lines[1:])  # Only keep header from first file\n",
    "\n",
    "    # Write output .a3m file named after the list file\n",
    "    out_path = os.path.join(os.path.dirname(list_file_path), f\"{base_name}.a3m\")\n",
    "    with open(out_path, 'w') as out:\n",
    "        for line in combined:\n",
    "            if line.startswith('>'):\n",
    "                out.write(f\">{base_name}_{line[1:]}\")\n",
    "            else:\n",
    "                out.write(line)\n",
    "\n",
    "    print(f\"Combined A3M file written to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def create_final_combined_file(list1_path, list2_path, a3m_directory):\n",
    "    \"\"\"\n",
    "    Combines the outputs of two list-based .a3m combinations into one final file.\n",
    "    \"\"\"\n",
    "    a3m1 = combine_a3m_files(list1_path, a3m_directory)\n",
    "    a3m2 = combine_a3m_files(list2_path, a3m_directory)\n",
    "\n",
    "    prefix = os.path.basename(list1_path).split('_')[0]\n",
    "    final_path = os.path.join(os.path.dirname(list1_path), f\"{prefix}_ForTree.fasta\")\n",
    "\n",
    "    with open(final_path, 'w') as fout:\n",
    "        with open(a3m1, 'r') as f1:\n",
    "            fout.writelines(f1.readlines())\n",
    "        with open(a3m2, 'r') as f2:\n",
    "            fout.writelines(f2.readlines()[2:])  # Skip second file's first record\n",
    "\n",
    "    print(f\"Final combined A3M file created at: {final_path}\")\n",
    "    return final_path\n",
    "\n",
    "# Run final combine\n",
    "final_combined_a3m = create_final_combined_file(list1_path, list2_path, a3m_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d361f9-18e8-4cd8-ade4-3928de45312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. fetch protein ID from uniprot.org of the combined fasta. \n",
    "\n",
    "import requests\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import glob\n",
    "\n",
    "####################################################################################################################10\n",
    "# fetch the protein ID with the previous output.\n",
    "####################################################################################################################\n",
    "# Function to fetch protein and organism information from UniProt, if the ID has\n",
    "# no hit in uniprot, no save in the ourput for downstream analysis. \n",
    "def fetch_uniprot_data(uniref_id):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/stream?query={uniref_id}&format=tsv&fields=protein_name,organism_name\"\n",
    "    headers = {\"User-Agent\": \"Python script\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        if len(lines) > 1:\n",
    "            data = lines[1].split(\"\\t\")\n",
    "            if len(data) >= 2:\n",
    "                protein_name = data[0]\n",
    "                organism_name = data[1]\n",
    "                print(f\"Fetched data for {uniref_id}: {protein_name}, {organism_name}\")\n",
    "                return protein_name, organism_name\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {uniref_id}. Status code: {response.status_code}\")\n",
    "        print(\"Response text:\")\n",
    "        print(response.text)\n",
    "        \n",
    "    print(f\"No data found for {uniref_id}.\")\n",
    "    return None, None\n",
    "\n",
    "# Function to update FASTA labels and exclude entries without UniProt data\n",
    "def update_fasta_labels(input_file):\n",
    "    print(f\"Processing {input_file}...\")\n",
    "    records = list(SeqIO.parse(input_file, \"fasta\"))\n",
    "    updated_records = []\n",
    "    \n",
    "    # Modify the description to replace underscores with dashes and fetch UniProt data\n",
    "    for record in records:\n",
    "        original_description = record.description\n",
    "        \n",
    "        # Look for UniRef100 ID and extract it\n",
    "        if \"UniRef100_\" in original_description:\n",
    "            uniref_id_part = original_description.split(\"UniRef100_\")[-1].split()[0].strip()  # Extract the UniRef ID part\n",
    "            \n",
    "            # Fetch data from UniProt\n",
    "            protein_name, organism_name = fetch_uniprot_data(uniref_id_part)\n",
    "            \n",
    "            # Only add records with valid UniProt data\n",
    "            if protein_name and organism_name:\n",
    "                record.description = f\"{protein_name} [{organism_name}] | {record.description}\"\n",
    "                updated_records.append(record)\n",
    "        else:\n",
    "            print(f\"No UniRef100 ID found in {record.id}, skipping.\")\n",
    "    \n",
    "    # Write the updated records to the output file if there are any\n",
    "    if updated_records:\n",
    "        output_file = input_file.replace('.fasta', '_TAX.fa')\n",
    "        SeqIO.write(updated_records, output_file, \"fasta\")\n",
    "        print(f\"Updated FASTA written to {output_file}\")\n",
    "    else:\n",
    "        print(f\"No valid records with UniProt data found in {input_file}.\")\n",
    "    return output_file\n",
    "\n",
    "TAX_FA = update_fasta_labels(final_combined_a3m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ec379-7a56-4715-9105-7c643b5b811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 generate summary three columns: protein, taxon, uniref100 ID\n",
    "# this is useful to set short names. \n",
    "import pandas as pd\n",
    "\n",
    "###########################################################################################################11\n",
    "# TAX_FA = final_combined_a3m.replace(\".fasta\",\"_TAX.fa\")\n",
    "############################################################################################################\n",
    "def process_fasta_file(fasta_file):\n",
    "    # Initialize lists to store the extracted information\n",
    "    ids = []\n",
    "    species_names = []\n",
    "    descriptions = []\n",
    "\n",
    "    # Read the fasta file line by line\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                # Extract the ID (first part before the first space or tab)\n",
    "                id_part = line.split()[0]\n",
    "                \n",
    "                # Extract the species name (string between [])\n",
    "                if '[' in line and ']' in line:\n",
    "                    species_name = line[line.find('[')+1 : line.find(']')]\n",
    "                else:\n",
    "                    species_name = \"Unknown\"  # Handle cases where species is missing\n",
    "                \n",
    "                # Extract the description (string between the first space/tab and first [)\n",
    "                if ' ' in line and '[' in line:\n",
    "                    description = line.split(' ', 1)[1].split('[')[0].strip()\n",
    "                else:\n",
    "                    description = \"No description\"  # Handle cases where description is missing\n",
    "                \n",
    "                # Append extracted data to lists\n",
    "                ids.append(id_part)\n",
    "                species_names.append(species_name)\n",
    "                descriptions.append(description)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'ID': ids,\n",
    "        'Species Name': species_names,\n",
    "        'Description': descriptions\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate the output CSV filename\n",
    "    output_file = fasta_file.replace('.fa', '_summary.csv').replace('.fasta', '_summary.csv')\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"CSV file saved as: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "process_fasta_file(TAX_FA)  # Uses input_fa_file from previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6e2c7-de06-46fe-8d0a-05c8f4f6829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.This is to replace the long ID to shorter ones for itol coloring \n",
    "\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "\n",
    "#############################################################################################################12\n",
    "# same input TAX_FA\n",
    "#############################################################################################################\n",
    "rename_conditions = {\n",
    "    #\"Drosophila\": \"umt\", \n",
    "    \"G-like protein ef-g2\": \"EF-G2\",\n",
    "   \"elongation factor g 1\": \"EF-G1\",\n",
    "    \"elongation factor g 2\": \"EF-G2\",\n",
    "    \"tetracycline\": \"Tetra\",\n",
    "    \"gtp-binding protein\": \"GtpB\",\n",
    "    \"p-loop containing\": \"Ploop\",\n",
    "    \"tr-type g\": \"TrG\",\n",
    "    \"elongation factor g\": \"EFG\",\n",
    "    \"elongation factor ef-g\": \"EFG\",\n",
    "    \"elongation factor efg\": \"EFG\",\n",
    "    \"EF-Tu\": \"Tu\",\n",
    "    \"elongation factor Tu\": \"Tu\",\n",
    "    \"EF-2\": \"EF2\",\n",
    "    \"elongation factor 4\": \"EF4\",\n",
    "    # Add more conditions as needed\n",
    "}\n",
    "\n",
    "# Function to rename sequence ID based on conditions\n",
    "def rename_sequence_id(sequence_id, rename_conditions):\n",
    "    # Extract the original identifier after the last space, tab, or pipe\n",
    "    # Split the ID into parts by space, tab, or pipe\n",
    "    parts = re.split(r'[ \\t|]+', sequence_id)\n",
    "    # Get the last relevant part\n",
    "    original_id = parts[-1]\n",
    "    \n",
    "    # Convert the sequence ID to lowercase for case-insensitive matching\n",
    "    lower_seq_id = sequence_id.lower()\n",
    "    \n",
    "    # Initialize the prefix as empty\n",
    "    prefix = \"\"\n",
    "    \n",
    "    # Check each condition in the rename_conditions dictionary\n",
    "    for term, rename in rename_conditions.items():\n",
    "        if term.lower() in lower_seq_id:\n",
    "            prefix = rename\n",
    "            break  # Stop at the first matching condition\n",
    "    \n",
    "    # Create the new short name\n",
    "    if prefix:\n",
    "        new_id = f\"{prefix}-{original_id}\"\n",
    "    else:\n",
    "        new_id = f\"{original_id}\"\n",
    "    \n",
    "    return new_id\n",
    "    \n",
    "# Automatically create the output file name by appending \"_shortened.fa\"\n",
    "output_fa_file = os.path.splitext(TAX_FA)[0] + \"_shortened.fas\"\n",
    "\n",
    "# Parse the input .fa file and rename sequence IDs\n",
    "with open(output_fa_file, \"w\") as output_handle:\n",
    "    for record in SeqIO.parse(TAX_FA, \"fasta\"):\n",
    "        original_id = record.description\n",
    "        new_id = rename_sequence_id(original_id, rename_conditions)\n",
    "        record.id = new_id\n",
    "        record.description = \"\"  # Remove the description after ID\n",
    "        SeqIO.write(record, output_handle, \"fasta\")\n",
    "\n",
    "print(f\"Renamed sequences saved to {output_fa_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b39b6-3843-4774-9c52-f73473898296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13, define tree methods.\n",
    "\n",
    "# make phylo tree with iqtree2\n",
    "def run_phylogenetic_tree(fas_file):\n",
    "    base_name = os.path.splitext(fas_file)[0]\n",
    "    # Command to run IQ-TREE for phylogenetic tree construction\n",
    "    command = [\n",
    "        'iqtree2',\n",
    "        '-s', fas_file,\n",
    "        '-m', 'TEST',\n",
    "        '-bb', '1000',\n",
    "        '-nt', 'AUTO',\n",
    "        '--prefix', base_name\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    print(f\"Phylogenetic tree constructed for {fas_file}\")\n",
    "\n",
    "# both tree\n",
    "def run_phylogenetic_and_ancestral(fas_file):\n",
    "    base_name = os.path.splitext(fas_file)[0]\n",
    "    # Command to run IQ-TREE for phylogenetic tree construction\n",
    "    phylo_command = [\n",
    "        'iqtree2',\n",
    "        '-s', fas_file,\n",
    "        '-m', 'TEST',\n",
    "        '-bb', '1000',\n",
    "        '-nt', 'AUTO',\n",
    "        '--prefix', base_name\n",
    "    ]\n",
    "    subprocess.run(phylo_command)\n",
    "    print(f\"Phylogenetic tree constructed for {fas_file}\")\n",
    "\n",
    "    # Command to run ancestral sequence reconstruction using the generated tree\n",
    "    ancestral_command = [\n",
    "        'iqtree2',\n",
    "        '-s', fas_file,\n",
    "        '-t', f\"{base_name}.treefile\",\n",
    "        '-nt', 'AUTO',\n",
    "        '--ancestral',\n",
    "        '--prefix', f\"{base_name}_anc\"\n",
    "    ]\n",
    "    subprocess.run(ancestral_command)\n",
    "    print(f\"Ancestral sequences reconstructed for {fas_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527fc2c-9cd3-4a27-b10c-76cd11f1eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 (check #14b for continuing on ancestral if interrupted. \n",
    "# Decide which process to run\n",
    "import glob\n",
    "import subprocess\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "###########################################################################################14\n",
    "# Set up input and options\n",
    "TAX_Shortened_FAS = TAX_FA.replace(\".fa\", \"_shortened.fas\")\n",
    "min_length = 700  # Overwrite if needed\n",
    "max_length = 710  # Overwrite if needed\n",
    "analysis_type = input(\"Enter 'tree' to run only the phylogenetic tree, or 'both' to run both tree and ancestral reconstruction: \").strip().lower()\n",
    "###########################################################################################\n",
    "\n",
    "# Filter sequences by length\n",
    "filtered_file = os.path.splitext(TAX_Shortened_FAS)[0] + \"_filtered.fas\"\n",
    "error_file = os.path.splitext(TAX_Shortened_FAS)[0] + \"_errors.txt\"\n",
    "\n",
    "valid_sequences = []\n",
    "error_log = []\n",
    "\n",
    "# Parse input FASTA and filter\n",
    "for record in SeqIO.parse(TAX_Shortened_FAS, \"fasta\"):\n",
    "    seq_length = len(record.seq)\n",
    "    if min_length <= seq_length <= max_length:\n",
    "        valid_sequences.append(record)\n",
    "    else:\n",
    "        error_log.append(\n",
    "            f\"ERROR: Sequence {record.id} contains {seq_length} characters, outside the range [{min_length}, {max_length}]\"\n",
    "        )\n",
    "\n",
    "# Write outputs\n",
    "SeqIO.write(valid_sequences, filtered_file, \"fasta\")\n",
    "with open(error_file, \"w\") as ef:\n",
    "    ef.write(\"\\n\".join(error_log))\n",
    "\n",
    "print(f\"Processed: {TAX_Shortened_FAS}\")\n",
    "print(f\"Filtered sequences saved to: {filtered_file}\")\n",
    "print(f\"Error log saved to: {error_file}\\n\")\n",
    "\n",
    "\n",
    "if analysis_type == 'both':\n",
    "    run_phylogenetic_and_ancestral(filtered_file)\n",
    "elif analysis_type == 'tree':\n",
    "    run_phylogenetic_tree(filtered_file)\n",
    "else:\n",
    "    print(\"Invalid input. Please enter 'tree' or 'both'.\")\n",
    "\n",
    "print(\"Requested analyses completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8954ff0-ceee-4ebb-856d-5499d65a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14b. run ancestral tree if phylotree already exist.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "#########################################################################################14b\n",
    "directory_for_tree =os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "##########################################################################################\n",
    "def find_filtered_treefile(directory):\n",
    "    \"\"\"Find the first *_filtered.treefile in the given directory.\"\"\"\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\"_filtered.treefile\"): # specify your target treefile name.\n",
    "            full_path = os.path.join(directory, file_name)\n",
    "            print(f\"Found treefile: {full_path}\")\n",
    "            return full_path  # Return the first match\n",
    "\n",
    "    print(\"No *_filtered.treefile found in the directory.\")\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Find the filtered tree file\n",
    "tree_source = find_filtered_treefile(directory_for_tree)\n",
    "\n",
    "if tree_source:\n",
    "    # Extract base name and directory\n",
    "    align_source = os.path.splitext(tree_source)[0] + \".fas\"\n",
    "    print(f\"Using alignment file: {align_source}\")\n",
    "\n",
    "    def run_ancestral_only(tree_file, align_file):\n",
    "        \"\"\"Run ancestral reconstruction using the existing tree file.\"\"\"\n",
    "        tree_dir = os.path.dirname(tree_file)  # Ensure output is stored in the same directory\n",
    "        base_name = os.path.splitext(os.path.basename(tree_file))[0]  # Get base filename\n",
    "        output_prefix = os.path.join(tree_dir, f\"{base_name}_anc\")  # Add _anc prefix\n",
    "\n",
    "        print(f\"Using treefile: {tree_file} for ancestral sequence reconstruction...\")\n",
    "        print(f\"Output will be saved as: {output_prefix}\")\n",
    "\n",
    "        # Run ancestral sequence reconstruction\n",
    "        ancestral_command = [\n",
    "            'iqtree2',\n",
    "            '-s', align_file,   # Use alignment file\n",
    "            '-t', tree_file,    # Use existing tree\n",
    "            '-nt', 'AUTO',\n",
    "            '--ancestral',\n",
    "            '--prefix', output_prefix  # Save outputs in the same directory with _anc suffix\n",
    "        ]\n",
    "        subprocess.run(ancestral_command)\n",
    "        print(f\"Ancestral sequences reconstructed in {tree_dir}\")\n",
    "\n",
    "    # Run ancestral reconstruction only if a treefile was found\n",
    "    run_ancestral_only(tree_source, align_source)\n",
    "else:\n",
    "    print(\"No tree file found. Exiting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa601c30-5a56-494c-ae66-e3af4732f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. # datasets for itol display of protein rings.\n",
    "from Bio import Phylo\n",
    "\n",
    "#########################################################################################15\n",
    "# Path to the directory containing .treefile files\n",
    "directory_path = os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "#######################################################################################\n",
    "# Define three different category_color mappings for proteins\n",
    "category_colors_A = {\n",
    "    \"EF2\": \"#ff0000\",  # red\n",
    "    \"EFG\": \"#c0c0c0\",  # silver grey\n",
    "}\n",
    "\n",
    "category_colors_B = {\n",
    "    \"EF-G2\": \"#00ffff\",  # cyan\n",
    "    \"Tetra\": \"#0000ff\",  # blue\n",
    "    \"EF-G1\":\"#ffd700\", #sunshine yellow\n",
    "}\n",
    "\n",
    "category_colors_C = {\n",
    "    \"Ploop\": \"#800080\",  # purple\n",
    "    \"GtpB\": \"#00ff00\",  # green\n",
    "    \"RF2\": \"#ffa500\",  # orange\n",
    "    \"TrG\": \"#ff69b4\",  # hot pink\n",
    "}\n",
    "\n",
    "def process_tree_file(tree_file, category_colors, dataset_label, output_suffix):\n",
    "    tree = Phylo.read(tree_file, 'newick')\n",
    "\n",
    "    # Note: Changed SEPARATOR to SPACE. Revert to TAB if SPACE is not supported by iTOL.\n",
    "    dataset_content = (f'DATASET_COLORSTRIP\\n'\n",
    "                       f'SEPARATOR SPACE\\n'\n",
    "                       f'DATASET_LABEL {dataset_label}\\n'\n",
    "                       f'COLOR #ff0000\\n'\n",
    "                       f'DATA\\n')\n",
    "\n",
    "    for label in tree.get_terminals():\n",
    "        label_name = label.name if label.name else \"\"\n",
    "        color = '#ffffff'  # Default color\n",
    "        for category, category_color in category_colors.items():\n",
    "            if category in label_name:\n",
    "                color = category_color\n",
    "                break\n",
    "        dataset_content += f'{label.name} {color}\\n'\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = os.path.splitext(tree_file)[0] + output_suffix + '.txt'\n",
    "    with open(output_file_path, 'w') as out_file:\n",
    "        out_file.write(dataset_content)\n",
    "    print(f\"iTOL dataset file created: {output_file_path}\")\n",
    "\n",
    "def process_all_treefiles(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('filtered.treefile'):\n",
    "            tree_file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing {tree_file_path}...\")\n",
    "\n",
    "            # Process for each category color mapping\n",
    "            process_tree_file(tree_file_path, category_colors_A, \"labelProA\", \"_itol_datasetA\")\n",
    "            process_tree_file(tree_file_path, category_colors_B, \"labelProB\", \"_itol_datasetB\")\n",
    "            process_tree_file(tree_file_path, category_colors_C, \"labelProC\", \"_itol_datasetC\")\n",
    "\n",
    "\n",
    "# Ensure the directory exists\n",
    "if os.path.isdir(directory_path):\n",
    "    process_all_treefiles(directory_path)\n",
    "else:\n",
    "    print(\"Invalid path or directory does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb2b00-8338-4ee0-bfdd-b1ffdbf3645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 generate the dataset for leave colors in itol.\n",
    "\n",
    "#########################################################################################16\n",
    "# Path to the directory containing .treefile files\n",
    "directory_path = os.path.dirname(TAX_Shortened_FAS) # all in the same folder of \"first_PSE_folder\"\n",
    "#######################################################################################\n",
    "# Define conditions for label background colors\n",
    "branch_colors_conditions = {\n",
    "    \"List1\": \"#ff0000\",  # red\n",
    "    \"List2\": \"#00ff00\",  # green\n",
    "    \"list1\": \"#ff0000\",  # red\n",
    "    \"list2\": \"#00ff00\",  # green\n",
    "    \"-1-\": \"#FF0000\",    # red\n",
    "    \"-2-\": \"#0000FF\",    # blue\n",
    "    \"-3-\": \"#FFFF00\",    # yellow\n",
    "    \"-4-\": \"#00FF00\",    # green\n",
    "    \"-7-\": \"#FF00FF\",    # magenta\n",
    "    \"-8-\": \"#00FFFF\",    # cyan\n",
    "    \"-13-\": \"#FFA500\",   # orange\n",
    "    \"-17-\": \"#800080\",   # purple\n",
    "    \"-24-\": \"#008000\",   # dark green\n",
    "    \"-28-\": \"#000080\",   # navy blue\n",
    "    \"-37-\": \"#FF4500\",   # orange-red\n",
    "    \"-137-\": \"#8B0000\",  # dark red\n",
    "    \"-248-\": \"#4682B4\",  # steel blue\n",
    "    # Add more conditions as needed\n",
    "}\n",
    "\n",
    "def generate_itol_branch_colors(tree_file):\n",
    "    \"\"\"Generate an iTOL TREE_COLORS dataset for branch colors.\"\"\"\n",
    "    tree = Phylo.read(tree_file, 'newick')\n",
    "    output_filename = os.path.splitext(tree_file)[0] + '_branch_colors.txt'\n",
    "    \n",
    "    with open(output_filename, 'w') as out_file:\n",
    "        out_file.write(\"TREE_COLORS\\n\")\n",
    "        out_file.write(\"#lines starting with a hash are comments and ignored during parsing\\n\")\n",
    "        out_file.write(\"SEPARATOR SPACE\\n\")\n",
    "        out_file.write(\"DATA\\n\")\n",
    "        \n",
    "        for clade in tree.find_clades():\n",
    "            if clade.name:\n",
    "                for keyword, color in branch_colors_conditions.items():\n",
    "                    if keyword in clade.name:\n",
    "                        # Write the branch color information, including style and width\n",
    "                        # Format: NODE_ID branch COLOR normal 1\n",
    "                        out_file.write(f\"{clade.name} branch {color} normal 1\\n\")\n",
    "                        break\n",
    "\n",
    "    print(f\"iTOL branch colors dataset generated: {output_filename}\")\n",
    "\n",
    "import os\n",
    "import glob  # Import the glob module to use glob.glob()\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Process all .treefile files in the specified directory to generate iTOL branch colors datasets.\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('filtered.treefile'):\n",
    "            tree_file = os.path.join(directory, filename)\n",
    "            print(f\"Processing {tree_file}...\")\n",
    "            generate_itol_branch_colors(tree_file)\n",
    "\n",
    "# Ensure the directory exists and process the files\n",
    "if os.path.isdir(directory_path):\n",
    "    process_directory(directory_path)\n",
    "else:\n",
    "    print(\"Invalid path or directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9369a9-9877-42ca-b9c9-7c12dfd9daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. pairwise comparison of msas to generate unique, and common clusters.\n",
    "\n",
    "################################################################################\n",
    "#it generates 1-unique, 2-unique, 1/2-common; then continue with 3 to generate 3x3 more txt, and so on.\n",
    "# For files with 0Kb, or other threshold, it will not get into the next round of clustering. \n",
    "# refer to Figures S4 and S10 in  \n",
    "################################################################################\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import shutil  # Import shutil for file operations\n",
    "\n",
    "def compare_sequence_files(file1_path, file2_path, output_dir):\n",
    "    \"\"\"\n",
    "    Compare sequences between two files and output unique and common sequences.\n",
    "    \"\"\"\n",
    "    file1_base = os.path.splitext(os.path.basename(file1_path))[0]\n",
    "    file2_base = os.path.splitext(os.path.basename(file2_path))[0]\n",
    "\n",
    "    # Parse sequences from both files\n",
    "    file1_seqs = {record.id: record for record in SeqIO.parse(file1_path, \"fasta\")}\n",
    "    file2_seqs = {record.id: record for record in SeqIO.parse(file2_path, \"fasta\")}\n",
    "\n",
    "    # Determine common and unique sequences\n",
    "    common_ids = set(file1_seqs.keys()) & set(file2_seqs.keys())\n",
    "    unique_file1_ids = set(file1_seqs.keys()) - common_ids\n",
    "    unique_file2_ids = set(file2_seqs.keys()) - common_ids\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # File paths for output\n",
    "    output_files = {\n",
    "        'common': os.path.join(output_dir, f\"{file2_base}{file1_base}.fasta\"),  # Concatenated name\n",
    "        'unique1': os.path.join(output_dir, f\"{file1_base}.fasta\"),  # Updated file1\n",
    "        'unique2': os.path.join(output_dir, f\"{file2_base}0.fasta\"),  # Updated file2\n",
    "    }\n",
    "\n",
    "    # Write outputs\n",
    "    for key, seq_ids in [('common', common_ids), ('unique1', unique_file1_ids), ('unique2', unique_file2_ids)]:\n",
    "        output_path = output_files[key]\n",
    "        with open(output_path, 'w') as output_handle:\n",
    "            if key == 'unique1':\n",
    "                SeqIO.write((file1_seqs[id_] for id_ in seq_ids), output_handle, \"fasta\")\n",
    "            elif key == 'unique2':\n",
    "                SeqIO.write((file2_seqs[id_] for id_ in seq_ids), output_handle, \"fasta\")\n",
    "            else:\n",
    "                SeqIO.write((file1_seqs[id_] for id_ in common_ids), output_handle, \"fasta\")\n",
    "\n",
    "        # Remove empty files\n",
    "        if os.path.getsize(output_path) == 0:\n",
    "            os.remove(output_path)\n",
    "            print(f\"Deleted empty file: {output_path}\")\n",
    "\n",
    "    # Overwrite file1 with unique sequences for next comparison\n",
    "    with open(file1_path, 'w') as output_handle:\n",
    "        SeqIO.write((file1_seqs[id_] for id_ in unique_file1_ids), output_handle, \"fasta\")\n",
    "\n",
    "    # Write a summary\n",
    "    summary_file = os.path.join(output_dir, f\"{file1_base}_{file2_base}_summary.txt\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Total sequences in {file1_base}: {len(file1_seqs)}\\n\")\n",
    "        f.write(f\"Total sequences in {file2_base}: {len(file2_seqs)}\\n\")\n",
    "        f.write(f\"Common sequences: {len(common_ids)}\\n\")\n",
    "        f.write(f\"Unique sequences in {file1_base}: {len(unique_file1_ids)}\\n\")\n",
    "        f.write(f\"Unique sequences in {file2_base}: {len(unique_file2_ids)}\\n\")\n",
    "\n",
    "    print(f\"Comparison completed for {file1_base} and {file2_base}. Outputs saved to {output_dir}.\")\n",
    "\n",
    "def process_files_in_rounds(file_list, base_folder):\n",
    "    \"\"\"\n",
    "    Process files sequentially, comparing each file with all files in the previous round's folder,\n",
    "    creating a new folder for each round.\n",
    "    \"\"\"\n",
    "    round_number = 1\n",
    "\n",
    "    # Use the first file as the initial reference\n",
    "    initial_file = file_list.pop(0)\n",
    "    current_round_folder = os.path.join(base_folder, f\"Combine_Round{round_number}\")\n",
    "    os.makedirs(current_round_folder, exist_ok=True)\n",
    "    shutil.copy(initial_file, os.path.join(current_round_folder, os.path.basename(initial_file)))\n",
    "\n",
    "    while file_list:\n",
    "        next_file = file_list.pop(0)\n",
    "        next_file_name = os.path.basename(next_file)\n",
    "        next_file_path = os.path.join(base_folder, next_file_name)\n",
    "\n",
    "        # Create the folder for the next round\n",
    "        next_round_folder = os.path.join(base_folder, f\"Combine_Round{round_number + 1}\")\n",
    "        os.makedirs(next_round_folder, exist_ok=True)\n",
    "\n",
    "        # Compare the new file with all files in the current round's folder\n",
    "        for fasta_file in os.listdir(current_round_folder):\n",
    "            if fasta_file.endswith('.fasta'):  # Only process FASTA files\n",
    "                fasta_path = os.path.join(current_round_folder, fasta_file)\n",
    "                compare_sequence_files(next_file_path, fasta_path, next_round_folder)\n",
    "\n",
    "        # Update the new file path after it is overwritten with unique sequences\n",
    "        next_file_path = os.path.join(next_round_folder, f\"{os.path.splitext(next_file_name)[0]}.fasta\")\n",
    "\n",
    "        # Move to the next round\n",
    "        current_round_folder = next_round_folder\n",
    "        round_number += 1\n",
    "\n",
    "# Example Usage\n",
    "combine_folder = \"/home/yuhong/demo/combine_All_MSA\"  # Path to the Combine folder\n",
    "file_list = [\n",
    "    \"/home/yuhong/demo/combine_All_MSA/1.fasta\",\n",
    "    \"/home/yuhong/demo/combine_All_MSA/2.fasta\",\n",
    "    \"/home/yuhong/demo/combine_All_MSA/3.fasta\",\n",
    "    \"/home/yuhong/demo/combine_All_MSA/4.fasta\",\n",
    "    \"/home/yuhong/demo/combine_All_MSA/7.fasta\",\n",
    "    \"/home/yuhong/demo/combine_All_MSA/8.fasta\",\n",
    "]\n",
    "\n",
    "process_files_in_rounds(file_list, combine_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d7fe2-a431-4a2f-9635-8dd4e6e30768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. calculate all nodes branch distance to the center after midpoint re-rooting,output the list. \n",
    "import os\n",
    "import csv\n",
    "from ete3 import Tree\n",
    "import glob\n",
    "##########################################################################18\n",
    "# input folder is first_PSE\n",
    "###########################################################################\n",
    "def extract_internal_nodes(anc_path):\n",
    "   # Find the first anc.treefile\n",
    "    ancfiles = glob.glob(os.path.join(anc_path, \"*anc.treefile\"))\n",
    "    if not ancfiles:\n",
    "        print(f\"❌ No 'anc.treefile' file found in {anc_path}.\")\n",
    "        return\n",
    "\n",
    "    ancfile = ancfiles[0]  # Take the first match\n",
    "    if not os.path.exists(ancfile):\n",
    "        print(f\"❌ anc file not found: {ancfile}\")\n",
    "        return\n",
    "\n",
    "    print(f\"📂 Using anc tree: {ancfile}\")\n",
    "\n",
    "    tree = Tree(ancfile, format=1)\n",
    "   \n",
    "    # Reroot the tree at the midpoint\n",
    "    tree.set_outgroup(tree.get_midpoint_outgroup())\n",
    "\n",
    "    # Extract all internal nodes (exclude leaves)\n",
    "    internal_nodes = [node for node in tree.traverse() if not node.is_leaf()]\n",
    "    \n",
    "    # Compute branch distances to the midpoint root\n",
    "    node_distances = {node.name: tree.get_distance(node) for node in internal_nodes if node.name}\n",
    "\n",
    "    # Define output CSV path\n",
    "    output_csv = os.path.join(anc_path, \"internal_nodes_distances.csv\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Node\", \"Distance_to_Midpoint\"])\n",
    "        for node, distance in node_distances.items():\n",
    "            writer.writerow([node, distance])\n",
    "\n",
    "    print(f\"Internal node distances saved to: {output_csv}\")\n",
    "    return output_csv\n",
    "    \n",
    "# Example usage\n",
    "node_branch_csv = extract_internal_nodes(first_PSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241b881-6932-4f85-891d-0c97e19bfda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. extract transposed sequence file of nodes selected below a threshold from #18,  \n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "######################################################################################19        \n",
    "disfile = node_branch_csv\n",
    "node_thr = 1.8\n",
    "######################################################################################\n",
    "def generate_node_list(csv_path, dis_thr):\n",
    "    \"\"\"\n",
    "    Filters nodes based on a distance threshold and saves the result in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file.\n",
    "    - dis_thr (float): Distance threshold for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - list: Filtered node names.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure correct column names\n",
    "    df.columns = [\"Node\", \"Distance_to_Midpoint\"]\n",
    "    \n",
    "    # Remove \"Node\" from each element in the Node column\n",
    "    df[\"Node\"] = df[\"Node\"].astype(str).str.replace(\"Node\", \"\", regex=False)\n",
    "    \n",
    "    # Filter nodes where distance < dis_thr\n",
    "    filtered_nodes = df[df[\"Distance_to_Midpoint\"] < dis_thr][\"Node\"].tolist()\n",
    "\n",
    "    # Format output\n",
    "    node_list_str = f\"node_list = {filtered_nodes}\"\n",
    "\n",
    "    # Save to a text file\n",
    "    output_file = csv_path.replace(\".csv\", f\"_nodes{dis_thr:.2f}.txt\")  # Format threshold nicely\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(node_list_str)\n",
    "\n",
    "    print(f\"✅ Node list saved to: {output_file}\")\n",
    "    print(node_list_str)  # Print for reference\n",
    "\n",
    "    return filtered_nodes  # Return the list instead of None\n",
    "\n",
    "def extract_full_matrices(state_path, nodes):\n",
    "    \"\"\"\n",
    "    Extracts the full probability matrix for each node from a .state file \n",
    "    and saves them as CSV files in the 'matrix' folder.\n",
    "    \"\"\"\n",
    "    nodes = [f\"Node{node}\" for node in nodes]  # Format node names\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    matrix_dir = os.path.join(state_path, \"matrix\")\n",
    "    os.makedirs(matrix_dir, exist_ok=True)\n",
    "    \n",
    "    # Find the first .state file\n",
    "    statefiles = glob.glob(os.path.join(state_path, \"*.state\"))\n",
    "    if not statefiles:\n",
    "        print(f\"❌ No 'state' file found in {state_path}.\")\n",
    "        return\n",
    "\n",
    "    statefile = statefiles[0]  # Take the first match\n",
    "    if not os.path.exists(statefile):\n",
    "        print(f\"❌ state file not found: {statefile}\")\n",
    "        return\n",
    "\n",
    "    print(f\"📂 Using state: {statefile}\")\n",
    "\n",
    "    # Read the file line by line\n",
    "    with open(statefile, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for node in nodes:\n",
    "        # Find the first occurrence of the exact node match\n",
    "        node_index = next((i for i, line in enumerate(lines) if line.split(\"\\t\")[0] == node), None)\n",
    "        if node_index is None:\n",
    "            print(f\"⚠️ No data found for {node}\")\n",
    "            continue\n",
    "\n",
    "        # Read all lines for the given node (skip header)\n",
    "        node_data = [line.strip().split(\"\\t\") for line in lines[node_index:] if line.split(\"\\t\")[0] == node]\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(node_data)\n",
    "\n",
    "        # Ensure the DataFrame has enough columns (skip if not enough data)\n",
    "        if df.shape[1] < 4:\n",
    "            print(f\"⚠️ Node {node} has insufficient columns, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Convert numerical values from the fourth column onward\n",
    "        df = df.iloc[:, 3:].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        # Transpose the matrix\n",
    "        df_transposed = df.T\n",
    "\n",
    "        # Save transposed matrix **without headers**\n",
    "        output_filename = os.path.join(matrix_dir, f\"{node}_matrix_transposed.csv\")\n",
    "        df_transposed.to_csv(output_filename, index=False, header=False)\n",
    "        print(f\"✅ Saved: {output_filename} (Transposed)\")\n",
    "\n",
    "    print(\"🎉 All requested node CSV files generated successfully!\")\n",
    "    return output_paths\n",
    "    \n",
    "# Generate node list\n",
    "node_list = generate_node_list(disfile,node_thr)\n",
    "\n",
    "# Pass the node list to extract_full_matrices\n",
    "node_matrix = extract_full_matrices(first_PSE, node_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f847718-de4c-490a-95b5-39ca75f7c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20. compute pairwise similarity of internal nodes\n",
    "# with JSD method. \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "##########################################################################20\n",
    "output_jsd_folder = os.path.join(matrix_dir, \"JSD\") # matrix_dir  \n",
    "final_output_filename = os.path.join(output_jsd_folder, \"node_JSD.csv\")\n",
    "###########################################################################\n",
    "def load_probability_matrix(file_path):\n",
    "    \"\"\"\n",
    "    Load a node's probability matrix from a CSV file without headers.\n",
    "    Assumes the file contains only raw probability values.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the probability matrix CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Probability matrix (20 amino acids × sequence length).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, header=None)  # Load CSV with no headers\n",
    "    return df.values  # Return probability matrix as a NumPy array\n",
    "\n",
    "\n",
    "def compute_jsd_matrix(input_dir, output_dir, final_output_file):\n",
    "    \"\"\"Compute pairwise Jensen-Shannon Divergence (JSD) and save site-specific matrices.\"\"\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except PermissionError:\n",
    "        print(f\"⚠️ Permission denied: Cannot write to {output_dir}. Using home directory instead.\")\n",
    "        output_dir = os.path.expanduser(\"~/JSD\")  # Switch to home directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        final_output_file = os.path.join(output_dir, \"node_JSD.csv\")\n",
    "\n",
    "    # Find all probability matrix files\n",
    "    files = glob.glob(os.path.join(input_dir, \"Node*_matrix_transposed.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No node probability matrix files found in {input_dir}\")\n",
    "\n",
    "    node_names = [os.path.basename(f).replace(\"_matrix_transposed.csv\", \"\") for f in files]\n",
    "    probability_matrices = {node: load_probability_matrix(f) for node, f in zip(node_names, files)}\n",
    "\n",
    "    num_positions = next(iter(probability_matrices.values())).shape[1]  # Sequence length\n",
    "    final_jsd_matrix = pd.DataFrame(index=node_names, columns=node_names, dtype=float)\n",
    "\n",
    "    # Compute JSD for each site\n",
    "    position_jsd_matrices = []\n",
    "    for k in range(num_positions):\n",
    "        position_jsd_matrix = pd.DataFrame(index=node_names, columns=node_names, dtype=float)\n",
    "\n",
    "        for i, node1 in enumerate(node_names):\n",
    "            for j, node2 in enumerate(node_names):\n",
    "                if i < j:\n",
    "                    p_vector = probability_matrices[node1][:, k]\n",
    "                    q_vector = probability_matrices[node2][:, k]\n",
    "                    jsd_value = jensenshannon(p_vector, q_vector)\n",
    "                    position_jsd_matrix.loc[node1, node2] = jsd_value\n",
    "                    position_jsd_matrix.loc[node2, node1] = jsd_value\n",
    "\n",
    "        # Save individual site JSD matrix\n",
    "        position_file = os.path.join(output_dir, f\"node_JSD_{k}.csv\")\n",
    "        position_jsd_matrix.to_csv(position_file)\n",
    "        position_jsd_matrices.append(position_jsd_matrix)\n",
    "\n",
    "    # Compute final average JSD matrix\n",
    "    for i, node1 in enumerate(node_names):\n",
    "        for j, node2 in enumerate(node_names):\n",
    "            if i < j:\n",
    "                mean_jsd = np.nanmean([pos.loc[node1, node2] for pos in position_jsd_matrices])\n",
    "                final_jsd_matrix.loc[node1, node2] = mean_jsd\n",
    "                final_jsd_matrix.loc[node2, node1] = mean_jsd\n",
    "\n",
    "    # Save final JSD results\n",
    "    final_jsd_matrix.to_csv(final_output_file)\n",
    "    print(f\"✅ Final JSD matrix saved to: {final_output_file}\")\n",
    "\n",
    "    return final_jsd_matrix\n",
    "\n",
    "# Example usage\n",
    "\n",
    "compute_jsd_matrix(matrix_dir, output_jsd_folder, final_output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baf5bf-1781-4bc2-b0f8-9b4225bccff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. use scipy Compute hierarchical clustering directly from df_summary (without squareform)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "##########################################################################################21\n",
    "# Define the path to the saved JSD matrix\n",
    "node_jsd_path = os.path.join(output_jsd_folder, \"node_JSD.csv\")\n",
    "\n",
    "# Load the saved JSD matrix. this is more reliable to read the .csv, which can be slow for large node list. \n",
    "if os.path.exists(node_jsd_path):\n",
    "    node_jsd = pd.read_csv(node_jsd_path, index_col=0)  # Load as DataFrame\n",
    "    print(\"✅ Successfully loaded node_JSD matrix without recomputing!\")\n",
    "\n",
    "    # Fix NaN values in the diagonal by replacing them with 1\n",
    "    np.fill_diagonal(node_jsd.values, 1)\n",
    "\n",
    "    # Overwrite the original file with the fixed matrix\n",
    "    node_jsd.to_csv(node_jsd_path)\n",
    "    print(f\"✅ Fixed and saved node_JSD matrix to: {node_jsd_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️ File not found: {node_jsd_path}\")\n",
    "\n",
    "##########################################################################################\n",
    "def hierarchical_clustering(JSD_input, plot_csv=True):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on a correlation summary CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - JSD_input (str): Path to the CSV file containing correlation summary.\n",
    "    - plot_csv (bool): Whether to generate a heatmap with dendrogram.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the saved clustered CSV file.\n",
    "    - str: Path to the saved dendrogram plot.\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    df_summary = pd.read_csv(JSD_input, index_col=0)\n",
    "\n",
    "    # Compute hierarchical clustering directly from df_summary (without squareform)\n",
    "    linkage_matrix = linkage(pdist(df_summary, metric='euclidean'), method='average')\n",
    "\n",
    "    # Get the SciPy cluster order\n",
    "    cluster_order = leaves_list(linkage_matrix)\n",
    "    ordered_labels_scipy = df_summary.index[cluster_order].tolist()\n",
    "    #print(\"🔹 SciPy Cluster Order:\", ordered_labels_scipy)\n",
    "\n",
    "    # Reorder the DataFrame based on clustering order\n",
    "    clustered_df = df_summary.iloc[cluster_order, :].iloc[:, cluster_order]\n",
    "    clustered_df.index = ordered_labels_scipy\n",
    "    clustered_df.columns = ordered_labels_scipy\n",
    "\n",
    "    # Save clustered CSV\n",
    "    clustered_csv_filename = JSD_input.replace(\".csv\", \"_cluster.csv\")\n",
    "    clustered_df.to_csv(clustered_csv_filename)\n",
    "    print(f\"✅ Clustered correlation summary saved as {clustered_csv_filename}\")\n",
    "\n",
    "    # Get the output directory\n",
    "    subfolder = os.path.dirname(JSD_input)\n",
    "\n",
    "    # Save dendrogram plot\n",
    "    dendro_plot_filename = os.path.join(subfolder, \"dendrogram.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dendrogram(linkage_matrix, labels=ordered_labels_scipy, leaf_rotation=90, leaf_font_size=10)\n",
    "    plt.title(\"Dendrogram of Correlation Summary Clustering\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.savefig(dendro_plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"✅ Dendrogram plot saved as {dendro_plot_filename}\")\n",
    "\n",
    "    # Plot heatmap with dendrogram if plot_csv is True\n",
    "    if plot_csv:\n",
    "        print(\"🔹 Running Seaborn clustermap (which performs its own clustering)...\")\n",
    "        g = sns.clustermap(df_summary, method='average', cmap='coolwarm', figsize=(12, 10))\n",
    "\n",
    "        # Extract Seaborn cluster order\n",
    "        reordered_rows_seaborn = [df_summary.index[i] for i in g.dendrogram_row.reordered_ind]\n",
    "        reordered_cols_seaborn = [df_summary.columns[i] for i in g.dendrogram_col.reordered_ind]\n",
    "\n",
    "       # print(\"🔹 Seaborn Cluster Order (Rows):\", reordered_rows_seaborn)\n",
    "       # print(\"🔹 Seaborn Cluster Order (Columns):\", reordered_cols_seaborn)\n",
    "\n",
    "        plt.title(\"Heatmap Dendrogram of Correlation Summary\")\n",
    "        plot_filename = os.path.join(subfolder, \"correlation_summary.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.show()\n",
    "        print(f\"✅ Correlation summary heatmap saved as {plot_filename}\")\n",
    "\n",
    "    return clustered_csv_filename, dendro_plot_filename\n",
    "\n",
    "# Run clustering function\n",
    "clustered_csv = hierarchical_clustering(node_jsd_path)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da6eaf-0f1f-4a26-8217-89e03dda56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. plot the dendrogram (clustered_csv from 25d) with customer color palette. the original plot as shown. \n",
    "# and plot the off-diagnal transition curve of element(i-1, i). \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "##################################################################################22\n",
    "#clustered_csv = hierarchical_clustering(node_jsd_path)[0] from 2\n",
    "##################################################################################\n",
    "\n",
    "def plot_clustered_heatmap(csv_file, color_map=None):\n",
    "    \"\"\"Plots a heatmap from a clustered CSV file with custom conditional coloring.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the clustered CSV file.\n",
    "    color_map (list of tuples): Custom color conditions [(threshold, color), ...].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default color mapping if not provided\n",
    "    if color_map is None:\n",
    "        color_map = [\n",
    "            (0.5, \"#FFFFFF\"),  # White for values >0.5\n",
    "            (0.3, \"#FFA500\"),  # Orange for 0.3 - 0.5\n",
    "            (0.0, \"#FF0000\")   # Red for < 0.3\n",
    "        ]\n",
    "    \n",
    "    # Load the clustered CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Extract thresholds and colors\n",
    "    thresholds, colors = zip(*sorted(color_map, key=lambda x: x[0]))  # Sort in ascending order\n",
    "    \n",
    "    # Define a colormap and corresponding normalization\n",
    "    cmap = mcolors.ListedColormap(colors)\n",
    "    norm = mcolors.BoundaryNorm(boundaries=list(thresholds) + [1.0], ncolors=len(colors) + 1)\n",
    "    \n",
    "    # Plot heatmap with fixed color mapping\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(df, annot=False, linewidths=0.5, cmap=cmap, norm=norm, cbar=True)\n",
    "    plt.title(\"Clustered Heatmap with Custom Coloring\")\n",
    "    \n",
    "    # Save plot\n",
    "    custom_plot_filename = csv_file.replace(\".csv\", \"_custom_heatmap.png\")\n",
    "    plt.savefig(custom_plot_filename)\n",
    "    plt.show()\n",
    "    print(f\"✅ Custom heatmap saved as {custom_plot_filename}\")\n",
    "    \n",
    "    return custom_plot_filename\n",
    "\n",
    "def plot_standard_clustermap(csv_file):\n",
    "    \"\"\"Plots the saved clustered CSV using the default clustermap settings.\"\"\"\n",
    "    \n",
    "    # Load the clustered CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "    # Plot using seaborn clustermap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    g = sns.clustermap(df, method='average', cmap='coolwarm', figsize=(12, 10))\n",
    "    plt.title(\"Heatmap Dendrogram of JSD Matrix\")\n",
    "    \n",
    "    # Save plot\n",
    "    clustermap_filename = csv_file.replace(\".csv\", \"_clustermap.png\")\n",
    "    plt.savefig(clustermap_filename)\n",
    "    plt.show()\n",
    "    print(f\"✅ Clustermap saved as {clustermap_filename}\")\n",
    "\n",
    "    return clustermap_filename\n",
    "\n",
    "def plot_off_diagonal_curve(csv_file, threshold=0.4):\n",
    "    \"\"\"Plot the off-diagonal elements above the diagonal elements as a single curve.\n",
    "    Identify clusters where off-diagonal values drop below the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the clustered CSV file.\n",
    "    threshold (float): Cutoff value to define clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the CSV file as a DataFrame\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    # Convert all data to numeric\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Ensure the matrix is square\n",
    "    if df.shape[0] != df.shape[1]:\n",
    "        raise ValueError(\"The input matrix must be square.\")\n",
    "    \n",
    "    # Extract the first off-diagonal elements (above the main diagonal)\n",
    "    off_diagonal_values = [df.iloc[i, i+1] for i in range(min(df.shape[0]-1, df.shape[1]-1))]\n",
    "    \n",
    "    # Plot the curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(off_diagonal_values) + 1), off_diagonal_values, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Off-Diagonal Value\")\n",
    "    plt.title(\"Off-Diagonal Elements Curve\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify clusters based on threshold\n",
    "    cluster_list = []\n",
    "    current_cluster = [df.index[0]]\n",
    "\n",
    "    for i in range(len(off_diagonal_values)):\n",
    "        if off_diagonal_values[i] > threshold:\n",
    "            if i > 0 and off_diagonal_values[i - 1] > threshold:\n",
    "                cluster_list.append(current_cluster)\n",
    "                current_cluster = [df.index[i+1]]\n",
    "            else:\n",
    "                cluster_list.append(current_cluster)\n",
    "                current_cluster = [df.index[i+1]]\n",
    "        else:\n",
    "            current_cluster.append(df.index[i+1])\n",
    "    \n",
    "    # Append the final cluster\n",
    "    if current_cluster:\n",
    "        cluster_list.append(current_cluster)\n",
    "    \n",
    "    # Save clusters to a text file\n",
    "    txt_filename = csv_file.replace(\".csv\", f\"_list{threshold}.txt\")\n",
    "    with open(txt_filename, \"w\") as f:\n",
    "        for idx, cluster in enumerate(cluster_list):\n",
    "            f.write(f\"Cluster {idx+1}: {', '.join(cluster)}\\n\")\n",
    "    \n",
    "    print(f\"✅ Cluster list saved as {txt_filename}\")\n",
    "\n",
    "# Run the heatmap and verification plots\n",
    "custom_heatmap = plot_clustered_heatmap(clustered_csv, color_map=None)\n",
    "clustermap_plot = plot_standard_clustermap(clustered_csv)\n",
    "off_diagonal_value = plot_off_diagonal_curve(clustered_csv, 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef91d1-9579-43c9-a218-4d929e327f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23. logomaker.\n",
    "# 1.this cell will make logo withOUT axis label, but enabling font and gap editing.\n",
    "#X-ticks are displayed. \n",
    "# also the valid_extension enalbe selectivity for processing. \n",
    "# will make a final combined png in the order of the range. \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import AlignIO\n",
    "import logomaker\n",
    "from PIL import Image\n",
    "\n",
    "###########################################################################23\n",
    "logo_dir = \"/home/yuhong/LOGO\"  # Path containing aligned sequences\n",
    "valid_extension = \".fasta\"  # Specify the desired extension to process\n",
    "start_pos = 1  # Start position (1-based index)\n",
    "end_pos = -1   # End position (inclusive, set to -1 for end of sequence)\n",
    "chunk_size = 40  # Size of each fragment\n",
    "width = 0.5  # Width of each letter; less than 1.0 increases spacing between letters\n",
    "font_weight = 'light'  # Font weight for the letters ('light', 'normal', 'bold', etc.)\n",
    "###########################################################################\n",
    "\n",
    "def create_sequence_logo(aligned_file, output_dir, start_pos, end_pos, width=0.5, font_weight='light'):\n",
    "    \"\"\"\n",
    "    Create a sequence logo for a specified range of residues and save it as a PNG file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        alignment = AlignIO.read(aligned_file, \"fasta\")\n",
    "        print(f\"Successfully read alignment file {aligned_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {aligned_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "    sequence_length = alignment.get_alignment_length()\n",
    "    start_pos = max(0, start_pos - 1)\n",
    "    end_pos = min(end_pos, sequence_length)\n",
    "\n",
    "    valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY-\")\n",
    "    fragment_alignment = []\n",
    "\n",
    "    for record in alignment:\n",
    "        seq = str(record.seq)[start_pos:end_pos]\n",
    "        sanitized = ''.join([aa if aa.upper() in valid_aa else '-' for aa in seq])\n",
    "        fragment_alignment.append(sanitized)\n",
    "        try:\n",
    "            fragment_df = logomaker.alignment_to_matrix(fragment_alignment, to_type='counts', characters_to_ignore='')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting alignment to matrix: {e}\")\n",
    "            return None\n",
    "\n",
    "    color_scheme = {\n",
    "        'A': 'green', 'C': 'blue', 'D': 'red', 'E': 'red',\n",
    "        'F': 'orange', 'G': 'orange', 'H': 'blue', 'I': 'green',\n",
    "        'K': 'red', 'L': 'green', 'M': 'green', 'N': 'blue',\n",
    "        'P': 'orange', 'Q': 'blue', 'R': 'red', 'S': 'orange',\n",
    "        'T': 'orange', 'V': 'green', 'W': 'orange', 'Y': 'orange',\n",
    "        '-': 'gray'\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    logomaker.Logo(fragment_df, ax=ax, color_scheme=color_scheme, width=width, font_name='DejaVu Sans', font_weight=font_weight)\n",
    "    ax.set_yticks([])  # Remove y-axis ticks\n",
    "    ax.set_xticks(fragment_df.index[::2])\n",
    "    ax.set_xticklabels(fragment_df.index[::2] + start_pos + 1)  # Adjust for correct numbering\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory {output_dir}\")\n",
    "\n",
    "    logo_path = os.path.join(output_dir, f\"{os.path.basename(aligned_file)}_logo_{start_pos + 1}to{end_pos}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(logo_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved logo to {logo_path}\")\n",
    "    return logo_path  # Return the path to include in the final combination\n",
    "\n",
    "\n",
    "def combine_images_vertically(image_paths, output_path):\n",
    "    \"\"\"\n",
    "    Combine images vertically and save the final combined image.\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        print(f\"⚠️ No images found for combination. Skipping {output_path}.\")\n",
    "        return\n",
    "\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "    max_width = max(img.width for img in images)\n",
    "    total_height = sum(img.height for img in images) + (len(images) * 30)  # Add spacing\n",
    "\n",
    "    combined_image = Image.new('RGB', (max_width, total_height), \"white\")\n",
    "    y_offset = 0\n",
    "\n",
    "    for img in images:\n",
    "        combined_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.height + 30\n",
    "\n",
    "    combined_image.save(output_path)\n",
    "    print(f\"✅ Combined image saved as: {output_path}\")\n",
    "\n",
    "\n",
    "def process_files_in_directory(directory_path, valid_extension='.fasta', start_pos=1, end_pos=-1, chunk_size=40, width=0.8, font_weight='light'):\n",
    "    \"\"\"\n",
    "    Process specific aligned sequence files in the directory based on a chosen extension and create sequence logos.\n",
    "    \"\"\"\n",
    "    fasta_files = [f for f in os.listdir(directory_path) if f.endswith(valid_extension)]\n",
    "    print(f\"Found {len(fasta_files)} {valid_extension} files in {directory_path}\")\n",
    "\n",
    "    if not fasta_files:\n",
    "        print(f\"No {valid_extension} files found in {directory_path}.\")\n",
    "        return\n",
    "\n",
    "    all_combined_images = []\n",
    "\n",
    "    for file_name in fasta_files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        logo_output_dir = os.path.join(directory_path, base_name)\n",
    "        \n",
    "        try:\n",
    "            alignment = AlignIO.read(file_path, \"fasta\")\n",
    "            sequence_length = alignment.get_alignment_length()\n",
    "            end_pos = end_pos if end_pos != -1 else sequence_length\n",
    "\n",
    "            chunk_start = start_pos\n",
    "            chunked_images = []\n",
    "\n",
    "            while chunk_start <= end_pos:\n",
    "                chunk_end = min(chunk_start + chunk_size - 1, end_pos)\n",
    "                logo_path = create_sequence_logo(file_path, logo_output_dir, chunk_start, chunk_end, width, font_weight)\n",
    "                \n",
    "                if logo_path:\n",
    "                    chunked_images.append(logo_path)\n",
    "                \n",
    "                chunk_start += chunk_size\n",
    "\n",
    "            # Combine images for this specific file into blocks (1-40, 41-80, ...)\n",
    "            if chunked_images:\n",
    "                combined_output = os.path.join(logo_output_dir, f\"{base_name}_combined.png\")\n",
    "                combine_images_vertically(chunked_images, combined_output)\n",
    "                all_combined_images.append(combined_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Final step: Merge all combined block images into a final PNG\n",
    "    if all_combined_images:\n",
    "        final_output_path = os.path.join(directory_path, \"final_combined_logo.png\")\n",
    "        combine_images_vertically(all_combined_images, final_output_path)\n",
    "        print(f\"✅ Final combined image saved: {final_output_path}\")\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "process_files_in_directory(logo_dir, valid_extension, start_pos, end_pos, chunk_size, width, font_weight)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
